{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b13a764",
   "metadata": {},
   "source": [
    "##### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2472f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface==0.1.2\n",
      "  Obtaining dependency information for langchain-huggingface==0.1.2 from https://files.pythonhosted.org/packages/9d/f8/77a303ddc492f6eed8bf0979f2bc6db4fa6eb1089c5e9f0f977dd87bc9c2/langchain_huggingface-0.1.2-py3-none-any.whl.metadata\n",
      "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.30.2)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.3.58)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface==0.1.2)\n",
      "  Obtaining dependency information for sentence-transformers>=2.6.0 from https://files.pythonhosted.org/packages/45/2d/1151b371f28caae565ad384fdc38198f1165571870217aedda230b9d7497/sentence_transformers-4.1.0-py3-none-any.whl.metadata\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.21.1)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface==0.1.2)\n",
      "  Obtaining dependency information for transformers>=4.39.0 from https://files.pythonhosted.org/packages/a9/b6/5257d04ae327b44db31f15cce39e6020cc986333c715660b1315a9724d82/transformers-4.51.3-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.13.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.11.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.15.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (11.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (0.5.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2025.4.26)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.6.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.3.1)\n",
      "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Installing collected packages: transformers, sentence-transformers, langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.1.2 sentence-transformers-4.1.0 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## 추가 패키지 설치\n",
    "pip install langchain-huggingface==0.1.2\n",
    "pip install langgraph==0.2.34\n",
    "pip install gradio==4.44.1\n",
    "!pip install langchain-community==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45e1596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adaptive_self_rag\n",
    "금융상품(예: 정기예금, 입출금자유예금) 관련 질의에 대해:\n",
    "1. 질문 라우팅 → (금융상품 관련이면) 문서 검색 (병렬 서브 그래프) → 문서 평가 → (조건부) 질문 재작성 → 답변 생성\n",
    "   / (금융상품과 무관하면) LLM fallback을 통해 바로 답변 생성\n",
    "그리고 생성된 답변의 품질(환각, 관련성) 평가 후 필요시 재생성 또는 재작성하는 Adaptive Self-RAG 체인.\n",
    "\"\"\"\n",
    "\n",
    "#############################\n",
    "# 1. 기본 환경 및 라이브러리\n",
    "#############################\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 기타 유틸\n",
    "import json\n",
    "import uuid\n",
    "from pprint import pprint\n",
    "from textwrap import dedent\n",
    "from operator import add\n",
    "\n",
    "\n",
    "# LangChain, Chroma, LLM 관련\n",
    "from typing import List, Literal, Sequence, TypedDict, Annotated, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Grader 평가지표용\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 그래프 관련\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Gradio 관련\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fece5f9",
   "metadata": {},
   "source": [
    "##### 2. 임베딩 및 DB설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ed524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# 2. 임베딩 및 DB 설정\n",
    "#############################\n",
    " \n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Chroma DB 경로\n",
    "CHROMA_DIR = \"./../findata/chroma_db\"\n",
    "\n",
    "# JSON 데이터 경로\n",
    "FIXED_JSON_PATH = \"./../findata/processed_fixed_deposit.json\"\n",
    "DEMAND_JSON_PATH = \"./../findata/processed_demand_deposit.json\"\n",
    "\n",
    "# DB 이름\n",
    "FIXED_COLLECTION = \"processed_fixed_deposit\"\n",
    "DEMAND_COLLECTION = \"processed_demand_deposit\"\n",
    "\n",
    "# 정기예금 DB\n",
    "fixed_deposit_db = Chroma(\n",
    "    embedding_function=embeddings_model,\n",
    "    collection_name=FIXED_COLLECTION,\n",
    "    persist_directory=CHROMA_DIR,\n",
    ")\n",
    "\n",
    "# 입출금자유예금 DB\n",
    "demand_deposit_db = Chroma(\n",
    "    embedding_function=embeddings_model,\n",
    "    collection_name=DEMAND_COLLECTION,\n",
    "    persist_directory=CHROMA_DIR,\n",
    ")\n",
    "\n",
    "\n",
    "# JSON -> Document 리스트 변환 함수\n",
    "def load_documents_from_json(json_path: str) -> list[Document]:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    documents = []\n",
    "    \n",
    "    for entry in data.get(\"documents\",[]):\n",
    "        content = entry.get(\"content\", \"\")\n",
    "        metadata = entry.get(\"metadata\", {})\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# 조건: DB가 비어 있으면 인제스천 (해당 조건이 없으면 계속 추가 됨..)\n",
    "if not fixed_deposit_db._collection.count():  # Chroma 내부 count()로 확인\n",
    "    print(\"[INFO] fixed_deposit DB is empty. Ingesting documents...\")\n",
    "    fixed_docs = load_documents_from_json(FIXED_JSON_PATH)\n",
    "    fixed_deposit_db.add_documents(fixed_docs)\n",
    "    print(f\"[INFO] {len(fixed_docs)} fixed deposit docs ingested.\")\n",
    "\n",
    "if not demand_deposit_db._collection.count():\n",
    "    print(\"[INFO] demand_deposit DB is empty. Ingesting documents...\")\n",
    "    demand_docs = load_documents_from_json(DEMAND_JSON_PATH)\n",
    "    demand_deposit_db.add_documents(demand_docs)\n",
    "    print(f\"[INFO] {len(demand_docs)} demand deposit docs ingested.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f6ba7",
   "metadata": {},
   "source": [
    "##### 3. 도구 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef46902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# 3. 도구(검색 함수) 정의\n",
    "#############################\n",
    "\n",
    "@tool\n",
    "def search_fixed_deposit(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Search for relevant fixed deposit (정기예금) product information using semantic similarity.\n",
    "    This tool retrieves products matching the user's query, such as interest rates or terms.\n",
    "    \"\"\"\n",
    "    docs = fixed_deposit_db.similarity_search(query, k=1)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    return [Document(page_content=\"관련 정기예금 상품정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_demand_deposit(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Search for demand deposit (입출금자유예금) product information using semantic similarity.\n",
    "    This tool retrieves products matching the user's query, such as flexible withdrawal or interest features.\n",
    "    \"\"\"\n",
    "    docs = demand_deposit_db.similarity_search(query, k=1)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    return [Document(page_content=\"관련 입출금자유예금 상품정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    This tool serves as a supplementary utility for the financial product recommendation model.\n",
    "    It retrieves up-to-date external information via web search using the Tavily API, \n",
    "    especially when relevant data is not available in the local vector databases\n",
    "\n",
    "    Unlike the RAG-based tools that query embedded product databases,\n",
    "    this tool is designed to handle broader or real-time questions—such as current interest rates, financial trends,\n",
    "    or general queries outside the scope of structured deposit data.\n",
    "\n",
    "    It returns the top 2 semantically relevant documents from the web.\n",
    "    \"\"\"\n",
    "@tool\n",
    "def web_search(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Tavily API로 웹 검색 후 Document 리스트 반환\n",
    "    \"\"\"\n",
    "    tavily_search = TavilySearchResults(max_results=2)\n",
    "\n",
    "    try:\n",
    "        docs = tavily_search.invoke(query)\n",
    "\n",
    "        # 전체가 str 하나일 경우: 에러 메시지로 처리\n",
    "        if isinstance(docs, str):\n",
    "            return [Document(page_content=f\"검색 실패: {docs}\", metadata={})]\n",
    "\n",
    "        # 문자열 리스트일 경우 (비정상 케이스일 수도 있음)\n",
    "        if isinstance(docs, list) and all(isinstance(d, str) for d in docs):\n",
    "            joined = \"\\n\".join(docs)\n",
    "            return [Document(page_content=joined, metadata={})]\n",
    "\n",
    "        # dict로 된 정상적인 검색 결과 처리\n",
    "        formatted_docs = []\n",
    "        for doc in docs:\n",
    "            if isinstance(doc, dict):\n",
    "                url = doc.get(\"url\", \"\")\n",
    "                content = doc.get(\"content\", \"내용 없음\")\n",
    "                formatted_docs.append(\n",
    "                    Document(\n",
    "                        page_content=f\"{content}\\n\\n 관련 링크: {url}\",\n",
    "                        metadata={\"source\": \"web_search\", \"url\": url}\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return formatted_docs or [Document(page_content=\"웹 검색 결과 없음\", metadata={})]\n",
    "\n",
    "    except Exception as e:\n",
    "        return [Document(page_content=f\"예외 발생: {str(e)}\", metadata={})]\n",
    "\n",
    "\n",
    "\n",
    "tools = [search_fixed_deposit, search_demand_deposit, web_search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff8277",
   "metadata": {},
   "source": [
    "##### 4. llm초기화 & 도구 바인딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bbaf7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# 4. LLM 초기화 & 도구 바인딩\n",
    "#############################\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39bb28",
   "metadata": {},
   "source": [
    "##### 5. LLM 체인 (Retrieval Grader / Answer Generator / Hallucination / Answer Graders / Question Re-writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================================================\n",
      " \n",
      "LLM 체인\n",
      "\n",
      "# (1) Retrieval Grader\n",
      "\n",
      "\n",
      "question : 어떤 예금 상품이 있는지 설명해주세요.\n",
      "\n",
      "검색된 문서 수: 2\n",
      "===============================================================================\n",
      "\n",
      "문서:\n",
      " 은행: 정보 없음\n",
      "상품명: 정보 없음\n",
      "기본금리(단리이자 %): 정보 없음\n",
      "최고금리(우대금리포함, 단리이자 %): 정보 없음\n",
      "은행 최종제공일: 정보 없음\n",
      "만기 후 금리: 정보 없음\n",
      "가입방법: 정보 없음\n",
      "우대조건: 정보 없음\n",
      "가입 제한조건: 정보 없음\n",
      "가입대상: 정보 없음\n",
      "기타 유의사항: 정보 없음\n",
      "최고한도: 정보 없음\n",
      "전월취급평균금리(만기 12개월 기준): 정보 없음\n",
      "---------------------------------------------------------------------------\n",
      "문서 관련성: binary_score='no'\n",
      "===========================================================================\n",
      "문서:\n",
      " 은행: BNK경남은행\n",
      "상품명: BNK더조은정기예금\n",
      "기본금리(단리이자 %): 2.55\n",
      "최고금리(우대금리포함, 단리이자 %): 3.05\n",
      "은행 최종제공일: 2025-01-24\n",
      "만기 후 금리: 만기 후 1개월 이내: 일반정기예금 기본이율 Ⅹ50%\n",
      "만기 후 1개월 초과: 일반정기예금 기본이율 Ⅹ20%\n",
      "가입방법: 인터넷뱅킹,스마트뱅킹\n",
      "우대조건: ①가입금액 20백만원 이상인 경우 0.20%\n",
      "②이 예금 신규시 금리우대쿠폰 등록할 경우 0.20% \n",
      "③ 경남은행 오픈뱅킹 서비스에 가입되어 있는 경우\n",
      "(만기시까지 해당서비스 유지하는 경우) 0.10%\n",
      "④ 자동재예치 신청 0.05%\n",
      "(단 금리우대쿠폰과 중복적용 불가)\n",
      "가입 제한조건: 제한없음\n",
      "가입대상: 거래대상자는 제한을 두지 아니한다. 다만, 국가 및 지방자치단체는 이 예금을 거래할 수 없다.\n",
      "기타 유의사항: 1. 이 예금의 계약기간은 3개월 이상 2년 이내 월단위로 한다.\n",
      "2. 가입금액은 1인당 최소 100만원 이상 5억원 이하이다.\n",
      "최고한도: 500,000,000원\n",
      "전월취급평균금리(만기 12개월 기준): 2.86\n",
      "---------------------------------------------------------------------------\n",
      "문서 관련성: binary_score='yes'\n",
      "===========================================================================\n",
      "\n",
      "# (2) Answer Generator (일반 RAG) \n",
      "\n",
      "Generated Answer (일반 RAG):\n",
      "BNK경남은행의 \"BNK더조은정기예금\" 상품이 있습니다. 이 예금 상품은 기본금리가 2.55%이며, 최고금리는 우대금리를 포함하여 3.05%입니다. 가입은 인터넷뱅킹과 스마트뱅킹을 통해 가능하며, 가입금액은 1인당 최소 100만원 이상 5억원 이하입니다. 계약기간은 3개월 이상 2년 이내로 설정할 수 있습니다. \n",
      "\n",
      "우대조건으로는 가입금액이 20백만원 이상인 경우 0.20%, 신규시 금리우대쿠폰 등록 시 0.20%, 경남은행 오픈뱅킹 서비스 가입 시 0.10%, 자동재예치 신청 시 0.05%의 금리 우대가 제공됩니다. 단, 금리우대쿠폰과 중복 적용은 불가합니다.\n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.\n",
      "\n",
      "# (3) Hallucination Grader\n",
      "\n",
      "환각 평가: binary_score='yes'\n",
      "\n",
      "# (4) Answer Grader\n",
      "\n",
      "Question: 어떤 예금 상품이 있는지 설명해주세요.\n",
      "Generation: BNK경남은행의 \"BNK더조은정기예금\" 상품이 있습니다. 이 예금 상품은 기본금리가 2.55%이며, 최고금리는 우대금리를 포함하여 3.05%입니다. 가입은 인터넷뱅킹과 스마트뱅킹을 통해 가능하며, 가입금액은 1인당 최소 100만원 이상 5억원 이하입니다. 계약기간은 3개월 이상 2년 이내로 설정할 수 있습니다. \n",
      "\n",
      "우대조건으로는 가입금액이 20백만원 이상인 경우 0.20%, 신규시 금리우대쿠폰 등록 시 0.20%, 경남은행 오픈뱅킹 서비스 가입 시 0.10%, 자동재예치 신청 시 0.05%의 금리 우대가 제공됩니다. 단, 금리우대쿠폰과 중복 적용은 불가합니다.\n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.\n",
      "답변 평가: binary_score='yes'\n",
      "\n",
      "# (5) Question Re-writer\n",
      "\n",
      "\n",
      "# (6) Generation Evaluation & Decision Nodes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# 5. LLM 체인 (Retrieval Grader / Answer Generator / Hallucination / Answer Graders / Question Re-writer)\n",
    "#############################\n",
    "print(\"\\n===================================================================\\n \")\n",
    "print(\"LLM 체인\\n\")\n",
    "print(\"# (1) Retrieval Grader\\n\")\n",
    "\n",
    "# (1) Retrieval Grader (검색평가)\n",
    "class BinaryGradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_BinaryGradeDocuments = llm.with_structured_output(BinaryGradeDocuments)\n",
    "\n",
    "system_prompt = \"\"\"You are an expert in evaluating the relevance of search results to user queries.\n",
    "\n",
    "[Evaluation criteria]\n",
    "1. 키워드 관련성: 문서가 질문의 주요 단어나 유사어를 포함하는지 확인\n",
    "2. 의미적 관련성: 문서의 전반적인 주제가 질문의 의도와 일치하는지 평가\n",
    "3. 부분 관련성: 질문의 일부를 다루거나 맥락 정보를 제공하는 문서도 고려\n",
    "4. 답변 가능성: 직접적인 답이 아니더라도 답변 형성에 도움될 정보 포함 여부 평가\n",
    "\n",
    "[Scoring]\n",
    "- Rate 'yes' if relevant, 'no' if not\n",
    "- Default to 'no' when uncertain\n",
    "\n",
    "[Key points]\n",
    "- Consider the full context of the query, not just word matching\n",
    "- Rate as relevant if useful information is present, even if not a complete answer\n",
    "\n",
    "Your evaluation is crucial for improving information retrieval systems. Provide balanced assessments.\n",
    "\"\"\"\n",
    "# 채점 프롬프트 템플릿\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"[Retrieved document]\\n{document}\\n\\n[User question]\\n{question}\")\n",
    "])\n",
    "\n",
    "retrieval_grader_binary = grade_prompt | structured_llm_BinaryGradeDocuments\n",
    "\n",
    "question = \"어떤 예금 상품이 있는지 설명해주세요.\"\n",
    "print(f'\\nquestion : {question}\\n')\n",
    "retrieved_docs = fixed_deposit_db.similarity_search(question, k=2)\n",
    "print(f\"검색된 문서 수: {len(retrieved_docs)}\")\n",
    "print(\"===============================================================================\")\n",
    "print()\n",
    "\n",
    "relevant_docs = []\n",
    "for doc in retrieved_docs:\n",
    "    print(\"문서:\\n\", doc.page_content)\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "\n",
    "    relevance = retrieval_grader_binary.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "    print(f\"문서 관련성: {relevance}\")\n",
    "\n",
    "    if relevance.binary_score == 'yes':\n",
    "        relevant_docs.append(doc)\n",
    "    \n",
    "    print(\"===========================================================================\")\n",
    "\n",
    "print(\"\\n# (2) Answer Generator (일반 RAG) \\n\")\n",
    "\n",
    "# (2) Answer Generator (일반 RAG)\n",
    "def generator_rag_answer(question, docs):\n",
    "\n",
    "    template = \"\"\"\n",
    "    [Your task]\n",
    "    You are a financial product expert and consultant who always responds in Korean.\n",
    "    Your task is to analyze the user query and the given financial product data to recommend the most suitable financial product.\n",
    "    \n",
    "    [Instructions]\n",
    "    1. 질문과 관련된 정보를 문맥에서 신중하게 확인합니다.\n",
    "    2. 답변에 질문과 직접 관련된 정보만 사용합니다.\n",
    "    3. 문맥에 명시되지 않은 내용에 대해 추측하지 않습니다.\n",
    "    4. 불필요한 정보를 피하고, 답변을 간결하고 명확하게 작성합니다.\n",
    "    5. 문맥에서 정확한 답변을 생성할 수 없다면 최대한 필요한 답변을 생성한 뒤 마지막에 \"더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.\"라고 덧붙여 답변합니다.\n",
    "    6. 적절한 경우 문맥에서 직접 인용하며, 따옴표를 사용합니다.\n",
    "\n",
    "    [Context]\n",
    "    {context}\n",
    "\n",
    "    [Question]\n",
    "    {question}\n",
    "\n",
    "    [Answer]\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    local_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        formatted_pdf = []\n",
    "        for d in docs:\n",
    "            content = d.page_content\n",
    "            if \"pdf_link\" in d.metadata:\n",
    "                content += f\"\\n 자세한 안내서: {d.metadata['pdf_link']}\"\n",
    "            formatted_pdf.append(content)\n",
    "        return \"\\n\\n\".join(formatted_pdf)\n",
    "    \n",
    "    rag_chain = prompt | local_llm | StrOutputParser()\n",
    "    generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "    return generation\n",
    "\n",
    "generation = generator_rag_answer(question, docs=relevant_docs)\n",
    "print(\"Generated Answer (일반 RAG):\")\n",
    "print(generation)\n",
    "\n",
    "# (3) Hallucination Grader\n",
    "print(\"\\n# (3) Hallucination Grader\\n\")\n",
    "\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_HradeHallucinations = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# 환각 평가를 위한 시스템 프롬프트 정의\n",
    "halluci_system_prompt = \"\"\"\n",
    "You are an expert evaluator assessing whether an LLM-generated answer is grounded in and supported by a given set of facts.\n",
    "\n",
    "[Your task]\n",
    "    - Review the LLM-generated answer.\n",
    "    - Determine if the answer is fully supported by the given facts.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - 답변에 주어진 사실이나 명확히 추론할 수 있는 정보 외의 내용이 없어야 합니다.\n",
    "    - 답변의 모든 핵심 내용이 주어진 사실에서 비롯되어야 합니다.\n",
    "    - 사실적 정확성에 집중하고, 글쓰기 스타일이나 완전성은 평가하지 않습니다.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer is factually grounded and fully supported.\n",
    "    - 'no': The answer includes information or claims not based on the given facts.\n",
    "\n",
    "Your evaluation is crucial in ensuring the reliability and factual accuracy of AI-generated responses. Be thorough and critical in your assessment.\n",
    "\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", halluci_system_prompt),\n",
    "        (\"human\", \"[Set of facts]\\n{documents}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_HradeHallucinations\n",
    "hallucination = hallucination_grader.invoke({\n",
    "    \"documents\": relevant_docs, \n",
    "    \"generation\": generation\n",
    "})\n",
    "print(f\"환각 평가: {hallucination}\")\n",
    "\n",
    "print(\"\\n# (4) Answer Grader\\n\")\n",
    "\n",
    "# (4) Answer Grader \n",
    "class BinaryGradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_BinaryGradeAnswer = llm.with_structured_output(BinaryGradeAnswer)\n",
    "grade_system_prompt = \"\"\"\n",
    "You are an expert evaluator tasked with assessing whether an LLM-generated answer effectively addresses and resolves a user's question.\n",
    "\n",
    "[Your task]\n",
    "    - Carefully analyze the user's question to understand its core intent and requirements.\n",
    "    - Determine if the LLM-generated answer sufficiently resolves the question.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - 관련성: 답변이 질문과 직접적으로 관련되어야 합니다.\n",
    "    - 완전성: 질문의 모든 측면이 다뤄져야 합니다.\n",
    "    - 정확성: 제공된 정보가 정확하고 최신이어야 합니다.\n",
    "    - 명확성: 답변이 명확하고 이해하기 쉬워야 합니다.\n",
    "    - 구체성: 질문의 요구 사항에 맞는 상세한 답변이어야 합니다.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer effectively resolves the question.\n",
    "    - 'no': The answer fails to sufficiently resolve the question or lacks crucial elements.\n",
    "\n",
    "Your evaluation plays a critical role in ensuring the quality and effectiveness of AI-generated responses. Strive for balanced and thoughtful assessments.\n",
    "\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"[User question]\\n{question}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader_binary = answer_prompt | structured_llm_BinaryGradeAnswer\n",
    "print(\"Question:\", question)\n",
    "print(\"Generation:\", generation)\n",
    "answer_score = answer_grader_binary.invoke({\"question\": question, \"generation\": generation})\n",
    "print(f\"답변 평가: {answer_score}\")\n",
    "\n",
    "\n",
    "print(\"\\n# (5) Question Re-writer\\n\")\n",
    "\n",
    "## 새로 추가한 함수 ==> 반복형 self-rag 답변 생성\n",
    "def generate_iteratively(state: \"SelfRagOverallState\") -> dict:\n",
    "    print(\"--- 반복형 Self-RAG 답변 생성 시작 ---\")\n",
    "\n",
    "    all_docs = state.get(\"documents\", []) + state.get(\"filtered_documents\", [])\n",
    "    unique_docs = list({doc.page_content: doc for doc in all_docs}.values())  # 중복 제거\n",
    "\n",
    "    best_answer = None\n",
    "    best_score = -1\n",
    "\n",
    "    for k in range(1, 8):\n",
    "        print(f\"--- [k={k}] 문서 누적 검색 ---\")\n",
    "        if len(unique_docs) < k:\n",
    "            new_docs = fixed_deposit_db.similarity_search(state[\"question\"], k=1)\n",
    "            for doc in new_docs:\n",
    "                if doc not in unique_docs:\n",
    "                    unique_docs.append(doc)\n",
    "\n",
    "        generation = generator_rag_answer(state[\"question\"], unique_docs)\n",
    "\n",
    "        temp_state = {\n",
    "            \"question\": state[\"question\"],\n",
    "            \"generation\": generation,\n",
    "            \"documents\": unique_docs,\n",
    "            \"num_generations\": k\n",
    "        }\n",
    "\n",
    "        result = grade_generation_self(temp_state)\n",
    "\n",
    "        if result == \"useful\":\n",
    "            print(\"--- 유용한 답변 발견 ---\")\n",
    "            return {\n",
    "                \"generation\": [generation],\n",
    "                \"documents\": unique_docs,\n",
    "                \"num_generations\": k\n",
    "            }\n",
    "\n",
    "        if result == \"not useful\":\n",
    "            best_answer = generation\n",
    "            best_score = 1\n",
    "\n",
    "    print(\"--- 최대 반복 도달 ---\")\n",
    "    return {\n",
    "        \"generation\": [best_answer if best_answer else \"문서 기반 답변을 생성할 수 없습니다.\"],\n",
    "        \"documents\": unique_docs,\n",
    "        \"num_generations\": 5\n",
    "    }\n",
    "\n",
    "\n",
    "# (5) Question Re-writer\n",
    "def rewrite_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    입력 질문을 벡터 검색에 최적화된 형태로 재작성한다.\n",
    "    \"\"\"\n",
    "    local_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert question re-writer. Your task is to convert input questions into optimized versions \n",
    "    for vectorstore retrieval. Analyze the input carefully and focus on capturing the underlying semantic \n",
    "    intent and meaning. Your goal is to create a question that will lead to more effective and relevant \n",
    "    document retrieval.\n",
    "\n",
    "    [Guidelines]\n",
    "        1. Identify and emphasize core concepts and key subjects.\n",
    "        2. Expand abbreviations or ambiguous terms.\n",
    "        3. Include synonyms or related terms that might appear in relevant documents.\n",
    "        4. Maintain the original intent and scope.\n",
    "        5. For complex questions, break them down into simpler, focused sub-questions.\n",
    "    \"\"\"\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"[Initial question]\\n{question}\\n\\n[Improved question]\\n\")\n",
    "    ])\n",
    "    question_rewriter = re_write_prompt | local_llm | StrOutputParser()\n",
    "    rewritten_question = question_rewriter.invoke({\"question\": question})\n",
    "    return rewritten_question\n",
    "\n",
    "print(\"\\n# (6) Generation Evaluation & Decision Nodes\\n\")\n",
    "\n",
    "# (6) Generation Evaluation & Decision Nodes\n",
    "def grade_generation_self(state: \"SelfRagOverallState\") -> str:\n",
    "    print(\"--- 답변 평가 (생성) ---\")\n",
    "    print(f\"--- 생성된 답변: {state['generation']} ---\")\n",
    "    if state['num_generations'] > 2:\n",
    "        print(\"--- 생성 2회 초과 → 반복형 생성으로 전환 ---\")\n",
    "        return \"generate_iteratively\"\n",
    "    # 평가를 위한 문서 텍스트 구성\n",
    "    print(\"--- 답변 할루시네이션 평가 ---\") \n",
    "    docs_text = \"\\n\\n\".join([d.page_content for d in state['documents']])\n",
    "    hallucination_grade = hallucination_grader.invoke({\n",
    "        \"documents\": docs_text,\n",
    "        \"generation\": state['generation']\n",
    "    })\n",
    "    if hallucination_grade.binary_score == \"yes\":\n",
    "        relevance_grade = retrieval_grader_binary.invoke({\n",
    "            \"question\": state['question'],\n",
    "            \"document\": state['generation']\n",
    "        })\n",
    "        print(\"--- 답변-질문 관련성 평가 ---\")\n",
    "        if relevance_grade.binary_score == \"yes\":\n",
    "            print(\"--- 생성된 답변이 질문을 잘 해결함 ---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"--- 답변 관련성이 부족 -> transform_query ---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "def decide_to_generate_self(state: \"SelfRagOverallState\") -> str:\n",
    "    print(\"--- 평가된 문서 분석 (초기 검색 후) ---\")\n",
    "    if not state['filtered_documents'] or len(state['filtered_documents']) < 2:\n",
    "        print(\"--- 관련 문서가 부족함 → 반복형 생성으로 전환 ---\")\n",
    "        return \"generate_iteratively\"\n",
    "    print(\"--- 문서 충분함 → 일반 생성 사용 ---\")\n",
    "    return \"generate\"\n",
    "\n",
    "\n",
    "# (7) RoutingDecision \n",
    "class RoutingDecision(BaseModel):\n",
    "    \"\"\"Determines whether a user question should be routed to document search or LLM fallback.\"\"\"\n",
    "    route: Literal[\"search_data\",\"llm_fallback\"] = Field(\n",
    "        description=\"Classify the question as 'search_data' (financial) or 'llm_fallback' (general)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec1e26",
   "metadata": {},
   "source": [
    "##### 6. 상태 정의 및 노드 함수 (전체 Adaptive 체인)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29163d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 통합: SelfRagOverallState (질문, 생성, 원본 문서, 필터 문서, 생성 횟수)\n",
    "#TODO\n",
    "\n",
    "# 메인 그래프 상태 정의\n",
    "class SelfRagOverallState(TypedDict):\n",
    "    \"\"\"\n",
    "    Adaptive Self-RAG 체인의 전체 상태를 관리    \n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: Annotated[List[str], add]\n",
    "    routing_decision: str = \"\" \n",
    "    num_generations: int = 0\n",
    "    documents: List[Document] = []\n",
    "    filtered_documents: List[Document] = []\n",
    "\n",
    "# 질문 재작성 노드 (변경 후 검색 루프)\n",
    "def transform_query_self(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- 질문 개선 ---\")\n",
    "    new_question = rewrite_question(state['question'])\n",
    "    print(f\"--- 개선된 질문 : \\n{new_question} \")\n",
    "    state['num_generations'] += 1\n",
    "    state['question'] = new_question  # 상태 업데이트\n",
    "    print(f\"num_generations : {state['num_generations']}\")\n",
    "    return {\"question\": new_question, \"num_generations\": state['num_generations']}\n",
    "\n",
    "# 답변 생성 노드 (서브 그래프로부터 받은 필터 문서 우선 사용)\n",
    "def generate_self(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- 답변 생성 ---\")\n",
    "    docs = state['filtered_documents'] if state['filtered_documents'] else state['documents']\n",
    "    generation = generator_rag_answer(state['question'], docs)\n",
    "    state['num_generations'] += 1\n",
    "    state['generation'] = generation\n",
    "    return {\n",
    "        \"generation\": [generation],         \n",
    "        \"num_generations\": state['num_generations'] + 1,\n",
    "    }\n",
    "\n",
    "\n",
    "structured_llm_RoutingDecision = llm.with_structured_output(RoutingDecision)\n",
    "\n",
    "question_router_system  = \"\"\"\n",
    "You are an AI assistant that routes user questions to the appropriate processing path.\n",
    "Return one of the following labels:\n",
    "- search_data\n",
    "- llm_fallback\n",
    "\"\"\"\n",
    "\n",
    "question_router_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", question_router_system),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "question_router = question_router_prompt | structured_llm_RoutingDecision\n",
    "\n",
    "# question route 노드 \n",
    "def route_question_adaptive(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- 질문 판단 (일반 or 금융) ---\")\n",
    "    print(f\"질문: {state['question']}\")\n",
    "    decision = question_router.invoke({\"question\": state['question']})\n",
    "    print(\"routing_decision:\", decision.route)\n",
    "    return {\"routing_decision\": decision.route}\n",
    "\n",
    "# question route 분기 함수 \n",
    "def route_question_adaptive_self(state: SelfRagOverallState) -> str:\n",
    "    \"\"\"\n",
    "    질문 분석 및 라우팅: 사용자의 질문을 분석하여 '금융질문'인지 '일반질문'인지 판단\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if state['routing_decision'] == \"llm_fallback\":\n",
    "            print(\"--- 일반질문으로 라우팅 ---\")\n",
    "            return \"llm_fallback\"\n",
    "        else:\n",
    "            print(\"--- 금융질문으로 라우팅 ---\")\n",
    "            return \"search_data\"\n",
    "    except Exception as e:\n",
    "        print(f\"--- 질문 분석 중 Exception 발생: {e} ---\")\n",
    "        return \"llm_fallback\"\n",
    "\n",
    "\n",
    "fallback_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    You are an AI assistant helping with various topics. \n",
    "    Respond in Korean.\n",
    "    - Provide accurate and helpful information.\n",
    "    - Keep answers concise yet informative.\n",
    "    - Inform users they can ask for clarification if needed.\n",
    "    - Let users know they can ask follow-up questions if needed.\n",
    "    - End every answer with the sentence: \"저는 금융상품 질문에 특화되어 있습니다. 금융상품관련 질문을 주세요.\"\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def llm_fallback_adaptive(state: SelfRagOverallState):\n",
    "    \"\"\"Generates a direct response using the LLM when the question is unrelated to financial products.\"\"\"\n",
    "    question = state['question']\n",
    "    fallback_chain = fallback_prompt | llm | StrOutputParser()\n",
    "    generation = fallback_chain.invoke({\"question\": question})\n",
    "    return {\"generation\": [generation]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2de9f1",
   "metadata": {},
   "source": [
    "##### 7. [서브 그래프 통합] - 병렬 검색 서브 그래프 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0921400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 상태 정의 (검색 서브 그래프 전용) ---\n",
    "class SearchState(TypedDict):\n",
    "    question: str\n",
    "    # generation: str\n",
    "    documents: Annotated[List[Document], add]  # 팬아웃된 각 검색 결과를 누적할 것\n",
    "    filtered_documents: List[Document]         # 관련성 평가를 통과한 문서들\n",
    "\n",
    "# ToolSearchState: SearchState에 추가 정보(datasources) 포함\n",
    "class ToolSearchState(SearchState):\n",
    "    datasources: List[str]  # 참조할 데이터 소스 목록\n",
    "\n",
    "# --- 서브그래프 노드 함수 ---\n",
    "def search_fixed_deposit_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    정기예금 상품 검색 (서브 그래프)\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print('--- 정기예금 상품 검색 --- ')\n",
    "    docs = search_fixed_deposit.invoke(question)\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 정기적금 상품정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "def search_demand_deposit_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    입출금자유예금 상품 검색 (서브 그래프)\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print('--- 입출금자유예금 상품 검색 ---')\n",
    "    docs = search_demand_deposit.invoke(question)\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 입출금자유예금 상품정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "def filter_documents_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    검색된 문서들에 대해 관련성 평가 후 필터링\n",
    "    \"\"\"\n",
    "    print(\"--- 문서 관련성 평가 (서브 그래프) ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader_binary.invoke({\n",
    "            \"question\": question,\n",
    "            \"document\": d.page_content\n",
    "        })\n",
    "        if score.binary_score == \"yes\":\n",
    "            print(\"--- 문서 관련성: 있음 ---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"--- 문서 관련성: 없음 ---\")\n",
    "    return {\"filtered_documents\": filtered_docs}\n",
    "\n",
    "def search_web_search_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    웹 검색 기반 금융 정보 검색 (서브 그래프)\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print('--- 웹 검색 실행 ---')\n",
    "\n",
    "    docs = web_search.invoke(question) \n",
    "\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 웹 정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "# --- 질문 라우팅 (서브 그래프 전용) ---\n",
    "class SubgraphToolSelector(BaseModel):\n",
    "    \"\"\"Selects the most appropriate tool for the user's question.\"\"\"\n",
    "    tool: Literal[\"search_fixed_deposit\", \"search_demand_deposit\", \"web_search\"] = Field(\n",
    "        description=\"Select one of the tools: search_fixed_deposit, search_demand_deposit or web_search based on the user's question.\"\n",
    "    )\n",
    "\n",
    "class SubgraphToolSelectors(BaseModel):\n",
    "    \"\"\"Selects all tools relevant to the user's question.\"\"\"\n",
    "    tools: List[SubgraphToolSelector] = Field(\n",
    "        description=\"Select one or more tools: search_fixed_deposit, search_demand_deposit or web_search based on the user's question.\"\n",
    "    )\n",
    "\n",
    "structured_llm_SubgraphToolSelectors = llm.with_structured_output(SubgraphToolSelectors)\n",
    "\n",
    "subgraph_system  = dedent(\"\"\"\\\n",
    "You are an AI assistant specializing in routing user questions to the appropriate tools.\n",
    "Use the following guidelines:\n",
    "- For fixed deposit product queries, use the search_fixed_deposit tool.\n",
    "- For demand deposit product queries, use the search_demand_deposit tool.\n",
    "- For general financial or real-time information queries, or when the user explicitly mentions 'web search',\n",
    "  use the web_search tool.\n",
    "  Always choose the appropriate tools based on the user's question.\n",
    "\"\"\")\n",
    "subgraph_route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", subgraph_system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "question_tool_router = subgraph_route_prompt  | structured_llm_SubgraphToolSelectors\n",
    "\n",
    "def analyze_question_tool_search(state: ToolSearchState):\n",
    "    \"\"\"\n",
    "    질문 분석 및 라우팅: 사용자의 질문에서 참조할 데이터 소스 결정\n",
    "    \"\"\"\n",
    "    print(\"--- 질문 라우팅 ---\")\n",
    "    question = state[\"question\"]\n",
    "    result = question_tool_router.invoke({\"question\": question})\n",
    "    datasources = [tool.tool for tool in result.tools]\n",
    "    return {\"datasources\": datasources}\n",
    "\n",
    "def route_datasources_tool_search(state: ToolSearchState) -> Sequence[str]:\n",
    "    \"\"\"\n",
    "    라우팅 결과에 따라 실행할 검색 노드를 결정 (병렬로 팬아웃)\n",
    "    \"\"\"\n",
    "    datasources = set(state['datasources'])\n",
    "\n",
    "    # 명확히 하나만 선택된 경우\n",
    "    if datasources == {'search_fixed_deposit'}:\n",
    "        return ['search_fixed_deposit']\n",
    "    elif datasources == {'search_demand_deposit'}:\n",
    "        return ['search_demand_deposit']\n",
    "    elif datasources == {'web_search'}:\n",
    "        return ['web_search']\n",
    "\n",
    "    # 도구가 전부 실행되거나 애매모호할 때는 도구 전부 실행\n",
    "    return ['search_fixed_deposit', 'search_demand_deposit', 'web_search']\n",
    "\n",
    "\n",
    "# --- 서브 그래프 빌더 구성 ---\n",
    "search_builder = StateGraph(ToolSearchState)\n",
    "\n",
    "# 노드 추가\n",
    "search_builder.add_node(\"analyze_question\", analyze_question_tool_search)\n",
    "search_builder.add_node(\"search_fixed_deposit\", search_fixed_deposit_subgraph)\n",
    "search_builder.add_node(\"search_demand_deposit\", search_demand_deposit_subgraph)\n",
    "search_builder.add_node(\"web_search\", search_web_search_subgraph)\n",
    "search_builder.add_node(\"filter_documents\", filter_documents_subgraph)\n",
    "\n",
    "# 엣지 구성\n",
    "search_builder.add_edge(START, \"analyze_question\")\n",
    "search_builder.add_conditional_edges(\n",
    "    \"analyze_question\",\n",
    "    route_datasources_tool_search,\n",
    "    {\n",
    "        \"search_fixed_deposit\": \"search_fixed_deposit\",\n",
    "        \"search_demand_deposit\": \"search_demand_deposit\",\n",
    "        \"web_search\": \"web_search\"\n",
    "    }\n",
    ")\n",
    "# 두 검색 노드 모두 실행한 후 각각의 결과는 filter_documents로 팬인(fan-in) 처리\n",
    "search_builder.add_edge(\"search_fixed_deposit\", \"filter_documents\")\n",
    "search_builder.add_edge(\"search_demand_deposit\", \"filter_documents\")\n",
    "search_builder.add_edge(\"web_search\", \"filter_documents\")\n",
    "search_builder.add_edge(\"filter_documents\", END)\n",
    "\n",
    "# 서브 그래프 컴파일\n",
    "tool_search_graph = search_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61732ee6",
   "metadata": {},
   "source": [
    "##### 8. [전체 그래프와 결합] - Self-RAG Overall Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ed831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 그래프 빌더 (rag_builder) 구성\n",
    "rag_builder = StateGraph(SelfRagOverallState)\n",
    "\n",
    "# 노드 추가: 검색 서브 그래프, 생성, 질문 재작성 등\n",
    "rag_builder.add_node(\"route_question\", route_question_adaptive)\n",
    "rag_builder.add_node(\"llm_fallback\", llm_fallback_adaptive)\n",
    "rag_builder.add_node(\"search_data\", tool_search_graph)         # 서브 그래프로 병렬 검색 및 필터링 수행\n",
    "rag_builder.add_node(\"generate\", generate_self)                # 답변 생성 노드\n",
    "rag_builder.add_node(\"generate_iteratively\", generate_iteratively)\n",
    "rag_builder.add_node(\"transform_query\", transform_query_self)  # 질문 개선 노드\n",
    "\n",
    "# 전체 그래프 엣지 구성\n",
    "rag_builder.add_edge(START, \"route_question\")\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"route_question\",\n",
    "    route_question_adaptive_self, \n",
    "    {\n",
    "        \"llm_fallback\": \"llm_fallback\",\n",
    "        \"search_data\": \"search_data\"\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_builder.add_edge(\"llm_fallback\", END)\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"search_data\",\n",
    "    decide_to_generate_self, \n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "        \"generate_iteratively\": \"generate_iteratively\",\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_builder.add_edge(\"transform_query\", \"search_data\")\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_self,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not supported\": \"generate\",      # 환각 발생 시 재생성\n",
    "        \"not useful\": \"transform_query\",  # 관련성 부족 시 질문 재작성 후 재검색\n",
    "        \"generate_iteratively\": \"generate_iteratively\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# generate_iteratively → 평가\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"generate_iteratively\",\n",
    "    grade_generation_self,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "        \"not supported\": END,\n",
    "        \"generate_iteratively\": \"generate_iteratively\",\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# MemorySaver 인스턴스 생성 (대화 상태를 저장할 in-memory 키-값 저장소)\n",
    "memory = MemorySaver()\n",
    "adaptive_self_rag_memory = rag_builder.compile(checkpointer=memory)\n",
    "# adaptive_self_rag = rag_builder.compile()\n",
    "\n",
    "# 그래프 파일 저장하기\n",
    "# display(Image(adaptive_self_rag.get_graph().draw_mermaid_png()))\n",
    "with open(\"adaptive_self_rag_memory.mmd\", \"w\") as f:\n",
    "    f.write(adaptive_self_rag_memory.get_graph(xray=True).draw_mermaid()) # 저장된 mmd 파일에서 코드 복사 후 https://mermaid.live 에 붙여넣기.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e81f2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 문서 1\n",
      "내용: 경제지표 뉴스 채권 채권 경제지표 뉴스 채권 1인당 국내총생산 채권 Apps App Store 경제지표 미국의 기준 금리는 마지막 기록으로 4.50%입니다. | 은행의 대차 대조표 | 23964.10 | 23911.30 | USD - 억 | Mar 2025 | | 외환 보유고 | 35208.00 | 34865.00 | USD - 백만 | Jan 2025 | | Fed Interest Rate | 4.50 | 4.50 | 퍼센트 | Mar 2025 | 뉴스 국내 총생산 국내 총생산 고정가격 국내총생산 1인당 국내총생산 챌린저 해고자 \n",
      "메타데이터: {'source': 'web_search', 'url': 'https://ko.tradingeconomics.com/united-states/interest-rate'}\n",
      "\n",
      "🔹 문서 2\n",
      "내용: 2025년 5월 기준, 24개월, 36개월 만기 은행 정기예금 이율 현황을 이어서 살펴보겠습니다. 앞서 6~12개월 만기 상품은 기본 이율 기준 연 2.50% 이상인\n",
      "\n",
      " 관련 링크: https://blog.naver.com/rbeod1/223849321321?fromRss=true&trackingCode=rss\n",
      "메타데이터: {'source': 'web_search', 'url': 'https://blog.naver.com/rbeod1/223849321321?fromRss=true&trackingCode=rss'}\n"
     ]
    }
   ],
   "source": [
    "docs = web_search.invoke(\"2025년 5월 최신 금리 정보를 알려줘\")\n",
    "for i, doc in enumerate(docs, start=1):\n",
    "    print(f\"\\n🔹 문서 {i}\")\n",
    "    print(\"내용:\", doc.page_content[:300])\n",
    "    print(\"메타데이터:\", doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3baa60",
   "metadata": {},
   "source": [
    "##### 9. Gradio Chatbot 구성 및 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d12fdd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://f6f8bb00b94241f36d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f6f8bb00b94241f36d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 질문 판단 (일반 or 금융) ---\n",
      "질문: 정기예금 상품 중 금리가 가장 높은 것은?\n",
      "routing_decision: search_data\n",
      "--- 금융질문으로 라우팅 ---\n",
      "--- 질문 라우팅 ---\n",
      "--- 정기예금 상품 검색 --- \n",
      "--- 문서 관련성 평가 (서브 그래프) ---\n",
      "--- 문서 관련성: 있음 ---\n",
      "--- 평가된 문서 분석 (초기 검색 후) ---\n",
      "--- 관련 문서가 부족함 → 반복형 생성으로 전환 ---\n",
      "--- 반복형 Self-RAG 답변 생성 시작 ---\n",
      "--- [k=1] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\n",
      "--- [k=2] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\n",
      "--- [k=3] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=4] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 \"더(The) 특판 정기예금\"입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=5] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로, 우대금리를 포함한 금리입니다. 따라서 이 상품이 정기예금 상품 중에서 금리가 가장 높은 것으로 보입니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=6] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=7] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 신규 고객에게 우대이율을 포함하여 최대 0.45%p의 추가 금리를 제공할 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 상품으로 추천드립니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- 최대 반복 도달 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: ['문서 기반 답변을 생성할 수 없습니다.'] ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1518, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\gradio\\utils.py\", line 793, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 623, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\giho0\\AppData\\Local\\Temp\\ipykernel_56808\\2525184982.py\", line 12, in chat\n",
      "    result = adaptive_self_rag_memory.invoke({\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1560, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1298, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py\", line 56, in tick\n",
      "    run_with_retry(t, retry_policy)\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 29, in run_with_retry\n",
      "    task.proc.invoke(task.input, config)\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 411, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 183, in invoke\n",
      "    ret = context.run(self.func, input, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\langgraph\\graph\\graph.py\", line 94, in _route\n",
      "    return self._finish(writer, input, result, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\langgraph\\graph\\graph.py\", line 129, in _finish\n",
      "    destinations: Sequence[Union[Send, str]] = [\n",
      "                                               ^\n",
      "  File \"c:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\langgraph\\graph\\graph.py\", line 130, in <listcomp>\n",
      "    r if isinstance(r, Send) else self.ends[r] for r in result\n",
      "                                  ~~~~~~~~~^^^\n",
      "KeyError: 'generate_iteratively'\n"
     ]
    }
   ],
   "source": [
    "# 챗봇 클래스\n",
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.thread_id = str(uuid.uuid4())\n",
    "\n",
    "    def chat(self, message: str, history: List[Tuple[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        입력 메시지와 대화 이력을 기반으로 Adaptive Self-RAG 체인을 호출하고,\n",
    "        응답을 반환합니다.\n",
    "        \"\"\"\n",
    "        config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
    "        result = adaptive_self_rag_memory.invoke({\n",
    "            \"question\": message,\n",
    "            \"num_generations\": 0 \n",
    "            },\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        gen_list = result.get(\"generation\", [])\n",
    "        bot_response = gen_list[-1] if gen_list else \"죄송합니다. 답변을 생성할 수 없습니다.\"\n",
    "\n",
    "        return bot_response\n",
    "\n",
    "\n",
    "# 챗봇 인스턴스 생성\n",
    "chatbot = ChatBot()\n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot.chat,\n",
    "    title=\"Adaptive Self-RAG 기반 RAG 챗봇 시스템\",\n",
    "    description=\"정기예금, 입출금자유예금 상품 및 기타 질문에 답변합니다.\",\n",
    "    examples=[\n",
    "        \"정기예금 상품 중 금리가 가장 높은 것은?\",\n",
    "        \"정기예금과 입출금자유예금은 어떤 차이점이 있나요?\",\n",
    "        \"은행의 예금 상품을 추천해 주세요.\"\n",
    "    ],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "# Gradio 앱 실행\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f788dc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "반환된 문서 수: 1개\n",
      "\n",
      "🔹 문서 1\n",
      "내용: 은행: Sh수협은행\n",
      "상품명: Sh평생주거래우대통장\n",
      "(잔액구간별)\n",
      "기본금리( %): 0.05\n",
      "최고금리(우대금리포함,  %): 0.2\n",
      "이자지급방식: 월지급\n",
      "은행 최종제공일: 2025-01-20\n",
      "가입방법: 영업점,스마트뱅킹\n",
      "우대조건: 기본조건 만족시 매일 최종잔액에 대하여 예금잔액구간에 따라 구간별금리 적용\n",
      "Ⅰ.매일최종잔액 100만원이하 : 0.05%\n",
      "Ⅱ.매일최종잔액 100만원초과 300만원이하 : 0.10%\n",
      "Ⅲ.매일최종잔액 300만원 초과 : 0.10%\n",
      "*우대금리\n",
      "-기본조건 충족시 Ⅱ구간에 대하여 0.10%우대적용\n",
      "-대상:CIF신규고객\n",
      "PDF 링크: http://localhost:8000/pdf/demand_deposit/수협_평생주거래우대통장.pdf\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 질문 판단 (일반 or 금융) ---\n",
      "질문: 안녕\n",
      "routing_decision: llm_fallback\n",
      "--- 일반질문으로 라우팅 ---\n",
      "--- 질문 판단 (일반 or 금융) ---\n",
      "질문: 정기예금 상품 중 금리가 가장 높은 것은?\n",
      "routing_decision: search_data\n",
      "--- 금융질문으로 라우팅 ---\n",
      "--- 질문 라우팅 ---\n",
      "--- 정기예금 상품 검색 --- \n",
      "--- 문서 관련성 평가 (서브 그래프) ---\n",
      "--- 문서 관련성: 있음 ---\n",
      "--- 평가된 문서 분석 (초기 검색 후) ---\n",
      "--- 관련 문서가 부족함 → 반복형 생성으로 전환 ---\n",
      "--- 반복형 Self-RAG 답변 생성 시작 ---\n",
      "--- [k=1] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\n",
      "--- [k=2] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\n",
      "--- [k=3] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=4] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=5] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 신규 고객에게 우대이율을 포함하여 최대 0.45%p의 추가 금리를 제공할 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 상품은 이 \"더(The) 특판 정기예금\"입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=6] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=7] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- 최대 반복 도달 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: ['안녕하세요! 어떻게 도와드릴까요? 궁금한 점이 있으면 말씀해 주세요. 저는 금융상품 질문에 특화되어 있습니다. 금융상품관련 질문을 주세요.', '문서 기반 답변을 생성할 수 없습니다.'] ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- 질문 판단 (일반 or 금융) ---\n",
      "질문: 정기예금과 입출금자유예금은 어떤 차이점이 있나요?\n",
      "routing_decision: llm_fallback\n",
      "--- 일반질문으로 라우팅 ---\n",
      "--- 질문 판단 (일반 or 금융) ---\n",
      "질문: 정기예금 상품 중 금리가 가장 높은 것은?\n",
      "routing_decision: search_data\n",
      "--- 금융질문으로 라우팅 ---\n",
      "--- 질문 라우팅 ---\n",
      "--- 정기예금 상품 검색 --- \n",
      "--- 문서 관련성 평가 (서브 그래프) ---\n",
      "--- 문서 관련성: 있음 ---\n",
      "--- 문서 관련성: 있음 ---\n",
      "--- 평가된 문서 분석 (초기 검색 후) ---\n",
      "--- 문서 충분함 → 일반 생성 사용 ---\n",
      "--- 답변 생성 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: ['안녕하세요! 어떻게 도와드릴까요? 궁금한 점이 있으면 말씀해 주세요. 저는 금융상품 질문에 특화되어 있습니다. 금융상품관련 질문을 주세요.', '정기예금과 입출금자유예금의 주요 차이점은 다음과 같습니다:\\n\\n1. **예치 기간**: 정기예금은 특정 기간(예: 1개월, 1년 등) 동안 돈을 예치해야 하며, 만기 전에 인출할 경우 이자 손실이 발생할 수 있습니다. 반면, 입출금자유예금은 언제든지 입출금이 가능하여 유동성이 높습니다.\\n\\n2. **이자율**: 정기예금은 일반적으로 입출금자유예금보다 높은 이자율을 제공합니다. 이는 자금을 일정 기간 동안 묶어두기 때문입니다.\\n\\n3. **목적**: 정기예금은 장기적인 자산 증식을 목표로 하는 경우에 적합하며, 입출금자유예금은 일상적인 자금 관리와 유동성을 중시하는 경우에 적합합니다.\\n\\n추가적인 질문이 필요하시면 언제든지 말씀해 주세요. 저는 금융상품 질문에 특화되어 있습니다. 금융상품관련 질문을 주세요.', 'BNK부산은행의 \"더(The) 특판 정기예금\" 상품이 금리가 가장 높은 정기예금입니다. 이 상품의 최고금리는 우대금리를 포함하여 3.2%입니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.'] ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\n",
      "--- 답변 생성 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: ['안녕하세요! 어떻게 도와드릴까요? 궁금한 점이 있으면 말씀해 주세요. 저는 금융상품 질문에 특화되어 있습니다. 금융상품관련 질문을 주세요.', '정기예금과 입출금자유예금의 주요 차이점은 다음과 같습니다:\\n\\n1. **예치 기간**: 정기예금은 특정 기간(예: 1개월, 1년 등) 동안 돈을 예치해야 하며, 만기 전에 인출할 경우 이자 손실이 발생할 수 있습니다. 반면, 입출금자유예금은 언제든지 입출금이 가능하여 유동성이 높습니다.\\n\\n2. **이자율**: 정기예금은 일반적으로 입출금자유예금보다 높은 이자율을 제공합니다. 이는 자금을 일정 기간 동안 묶어두기 때문입니다.\\n\\n3. **목적**: 정기예금은 장기적인 자산 증식을 목표로 하는 경우에 적합하며, 입출금자유예금은 일상적인 자금 관리와 유동성을 중시하는 경우에 적합합니다.\\n\\n추가적인 질문이 필요하시면 언제든지 말씀해 주세요. 저는 금융상품 질문에 특화되어 있습니다. 금융상품관련 질문을 주세요.', 'BNK부산은행의 \"더(The) 특판 정기예금\" 상품이 금리가 가장 높은 정기예금입니다. 이 상품의 최고금리는 우대금리를 포함하여 3.2%입니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.', 'BNK부산은행의 \"더(The) 특판 정기예금\" 상품이 금리가 가장 높은 정기예금입니다. 이 상품의 최고금리는 우대금리를 포함하여 3.2%입니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.'] ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- 반복형 Self-RAG 답변 생성 시작 ---\n",
      "--- [k=1] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\n",
      "--- [k=2] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로, 우대금리를 포함한 금리입니다. 따라서 이 상품이 정기예금 상품 중에서 금리가 가장 높은 것으로 보입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\n",
      "--- [k=3] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 신규 고객에게 우대이율을 포함하여 최대 0.45%p의 추가 금리를 제공할 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 상품은 \"더(The) 특판 정기예금\"입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=4] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=5] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=6] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- [k=7] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: BNK부산은행의 \"더(The) 특판 정기예금\" 상품은 최고금리가 3.2%로 제공됩니다. 이 상품은 우대금리를 포함한 금리로, 신규 고객 우대이율과 이벤트 우대이율을 통해 최대 0.45%p의 추가 금리를 받을 수 있습니다. 따라서, 정기예금 상품 중에서 금리가 가장 높은 것은 이 상품입니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n",
      "--- 최대 반복 도달 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: ['안녕하세요! 어떻게 도와드릴까요? 궁금한 점이 있으면 말씀해 주세요. 저는 금융상품 질문에 특화되어 있습니다. 금융상품관련 질문을 주세요.', '정기예금과 입출금자유예금의 주요 차이점은 다음과 같습니다:\\n\\n1. **예치 기간**: 정기예금은 특정 기간(예: 1개월, 1년 등) 동안 돈을 예치해야 하며, 만기 전에 인출할 경우 이자 손실이 발생할 수 있습니다. 반면, 입출금자유예금은 언제든지 입출금이 가능하여 유동성이 높습니다.\\n\\n2. **이자율**: 정기예금은 일반적으로 입출금자유예금보다 높은 이자율을 제공합니다. 이는 자금을 일정 기간 동안 묶어두기 때문입니다.\\n\\n3. **목적**: 정기예금은 장기적인 자산 증식을 목표로 하는 경우에 적합하며, 입출금자유예금은 일상적인 자금 관리와 유동성을 중시하는 경우에 적합합니다.\\n\\n추가적인 질문이 필요하시면 언제든지 말씀해 주세요. 저는 금융상품 질문에 특화되어 있습니다. 금융상품관련 질문을 주세요.', 'BNK부산은행의 \"더(The) 특판 정기예금\" 상품이 금리가 가장 높은 정기예금입니다. 이 상품의 최고금리는 우대금리를 포함하여 3.2%입니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.', 'BNK부산은행의 \"더(The) 특판 정기예금\" 상품이 금리가 가장 높은 정기예금입니다. 이 상품의 최고금리는 우대금리를 포함하여 3.2%입니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.', '문서 기반 답변을 생성할 수 없습니다.'] ---\n",
      "--- 생성 2회 초과 → 반복형 생성으로 전환 ---\n"
     ]
    }
   ],
   "source": [
    "query = \"산업은행 적금상품 추천해줘\"\n",
    "docs = search_demand_deposit.invoke(query)\n",
    "\n",
    "print(f\"\\n반환된 문서 수: {len(docs)}개\\n\")\n",
    "\n",
    "for i, d in enumerate(docs, start=1):\n",
    "    print(f\"🔹 문서 {i}\")\n",
    "    print(\"내용:\", d.page_content[:300])  # 길면 자르기\n",
    "    print(\"PDF 링크:\", d.metadata.get(\"pdf_link\", \"❌ 없음\"))\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eefd7c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57d052cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 질문 판단 (일반 or 금융) ---\n",
      "질문: 입출금이 자유로운 예금 상품을 추천해주세요.\n",
      "routing_decision: search_data\n",
      "--- 금융질문으로 라우팅 ---\n",
      "--- 질문 라우팅 ---\n",
      "--- 입출금자유예금 상품 검색 ---\n",
      "--- 문서 관련성 평가 (서브 그래프) ---\n",
      "--- 문서 관련성: 없음 ---\n",
      "--- 평가된 문서 분석 ---\n",
      "--- 문서 부족 → 반복형 생성 ---\n",
      "--- 반복형 Self-RAG 답변 생성 시작 ---\n",
      "--- [k=1] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: Sh수협은행의 \"Sh내가만든통장\"은 입출금이 자유로운 예금 상품으로 적합합니다. 이 상품은 기본금리가 0.1%이며, 최대 금리는 우대금리를 포함해 1.0%입니다. 이자는 월 지급되며, 가입 방법은 영업점이나 스마트뱅킹을 통해 가능합니다. \n",
      "\n",
      "가입 시 최소 지정금액은 1천만원이며, 최대 고객 지정금액은 10억원입니다. 이 상품은 실명의 개인에게 제한 없이 가입할 수 있으며, 1인 1계좌로 운영됩니다. \n",
      "\n",
      "입출금이 자유로운 조건을 원하신다면 이 상품이 적합할 것입니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\n",
      "--- [k=2] 문서 누적 검색 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: 입출금이 자유로운 예금 상품으로는 Sh수협은행의 \"Sh내가만든통장\"을 추천드립니다. 이 상품은 기본금리가 0.1%이며, 최대 금리는 1.0%로 우대금리를 포함하고 있습니다. 이자 지급 방식은 월 지급이며, 가입 방법은 영업점이나 스마트뱅킹을 통해 가능합니다. \n",
      "\n",
      "가입 시 최소 지정금액은 1천만원이며, 최대 고객 지정금액은 10억원입니다. 이 상품은 실명의 개인이 가입할 수 있으며, 가입 제한 조건은 없습니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다. ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 답변-질문 관련성 평가 ---\n",
      "--- 생성된 답변이 질문을 잘 해결함 ---\n",
      "--- 유용한 답변 발견 ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: ['입출금이 자유로운 예금 상품으로는 Sh수협은행의 \"Sh내가만든통장\"을 추천드립니다. 이 상품은 기본금리가 0.1%이며, 최대 금리는 1.0%로 우대금리를 포함하고 있습니다. 이자 지급 방식은 월 지급이며, 가입 방법은 영업점이나 스마트뱅킹을 통해 가능합니다. \\n\\n가입 시 최소 지정금액은 1천만원이며, 최대 고객 지정금액은 10억원입니다. 이 상품은 실명의 개인이 가입할 수 있으며, 가입 제한 조건은 없습니다. \\n\\n더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.'] ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 답변-질문 관련성 평가 ---\n",
      "--- 생성된 답변이 질문을 잘 해결함 ---\n",
      "\n",
      "✅ 최종 생성된 답변:\n",
      "\n",
      "입출금이 자유로운 예금 상품으로는 Sh수협은행의 \"Sh내가만든통장\"을 추천드립니다. 이 상품은 기본금리가 0.1%이며, 최대 금리는 1.0%로 우대금리를 포함하고 있습니다. 이자 지급 방식은 월 지급이며, 가입 방법은 영업점이나 스마트뱅킹을 통해 가능합니다. \n",
      "\n",
      "가입 시 최소 지정금액은 1천만원이며, 최대 고객 지정금액은 10억원입니다. 이 상품은 실명의 개인이 가입할 수 있으며, 가입 제한 조건은 없습니다. \n",
      "\n",
      "더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "test_question = \"입출금이 자유로운 예금 상품을 추천해주세요.\"\n",
    "\n",
    "initial_state = {\n",
    "    \"question\": test_question,\n",
    "    \"generation\": [],\n",
    "    \"routing_decision\": \"\",\n",
    "    \"num_generations\": 0,\n",
    "    \"documents\": [],\n",
    "    \"filtered_documents\": []\n",
    "}\n",
    "\n",
    "# ✅ thread_id 추가로 checkpointer 오류 방지\n",
    "final_state = adaptive_self_rag_memory.invoke(\n",
    "    initial_state,\n",
    "    config={\"thread_id\": \"test-thread\"}\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n✅ 최종 생성된 답변:\\n\")\n",
    "print(final_state[\"generation\"][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
